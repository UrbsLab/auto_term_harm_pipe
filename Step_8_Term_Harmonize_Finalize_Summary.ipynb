{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Harmonize - STEP 8: Finalize and Summarize Mapping File\n",
    "#### Author: Ryan Urbanowicz (ryanurb@upenn.edu) \n",
    "#### Institution: University of Pennsylvania - Perleman School of Medicine\n",
    "#### Project: CMREF Data Harmonization \n",
    "#### Date: 7/18/19\n",
    "\n",
    "#### Project Overview:\n",
    "See the first notebook in this series ('Step_1_Term_Harmonize_Data_Preparation.ipynb') for an overview of this project, these notebooks, the target application, data availability, code dependencies, and our strategy for generalizing the code in these notebooks. \n",
    "\n",
    "#### Notebook Summary:\n",
    "This notebook will take the working file which has added imputed SOCs to the rest of the mapping and will then map all these unique rows back to the original target dataset from the first notebook.  This notebook will also perform a quality control check, summary, and final formatting of the mapping file for the target application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Load Python packages required in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#Load necessary packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Jupyter Notebook Hack: This code ensures that the results of multiple commands within a given cell are all displayed, rather than just the last. \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#Import Progress bar\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create general variable names for any target application specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input filename for 'target dataset' (excel file loaded in this application)\n",
    "target_study_data = 'Combined_MEDHX_TERMS_20studies.xlsx' \n",
    "\n",
    "ont_DL1_data = 'LLT.xlsx' # Input filename for ontology file defining all DL1 terms and their codes. \n",
    "ont_DL1_name_col = 'llt_name' # column label for DL1 term name\n",
    "ont_DL1_code_col ='llt_code' # column label for DL1 term code\n",
    "ont_DL1_cur_col = 'llt_currency' # column label for term currency\n",
    "\n",
    "ont_DL2_data = 'PT.xlsx' # Input filename for ontology file defining all DL2 terms and their codes. \n",
    "ont_DL2_name_col = 'pt_name' # column label for DL2 term name\n",
    "ont_DL2_code_col = 'pt_code' # column label for DL2 term code\n",
    "\n",
    "ont_DL3_DL2_data = 'HLT_PT.xlsx' # Input filename for ontology file defining connections between DL2 and DL3 term codes. \n",
    "ont_DL3_data = 'HLT.xlsx' # Input filename for ontology file defining all DL3 terms and their codes.\n",
    "ont_DL3_name_col = 'hlt_name' # column label for DL3 term name\n",
    "ont_DL3_code_col = 'hlt_code' # column label for DL3 term code\n",
    "\n",
    "ont_DL4_DL3_data = 'HLGT_HLT.xlsx' # Input filename for ontology file defining connections between DL3 and DL4 term codes. \n",
    "ont_DL4_data = 'HLGT.xlsx' # Input filename for ontology file defining all DL4 terms and their codes.\n",
    "ont_DL4_name_col = 'hlgt_name' # column label for DL4 term name\n",
    "ont_DL4_code_col = 'hlgt_code' # column label for DL4 term code\n",
    "\n",
    "ont_DL5_DL4_data = 'SOC_HLGT.xlsx' # Input filename for ontology file defining connections between DL4 and DL5 term codes. \n",
    "ont_DL5_data = 'SOC.xlsx' # Input filename for ontology file defining all DL5 terms and their codes.\n",
    "ont_DL5_name_col = 'soc_name' # column label for DL5 term name\n",
    "ont_DL5_code_col = 'soc_code' # column label for DL5 term code\n",
    "\n",
    "DL1_FT1 = 'MHTERM' # focus term 1: This term is available over all studies. \n",
    "DL1_FT2 = 'LLT_NAME' # focus term 3: an alternative term available for a subset of studies. This one supposedly conforms to the MedDRA standard so we expect it to yield more exact matches. May offer a better match for the lowest level of the standardized terminology.\n",
    "DL1_FT3 = 'MHMODIFY' # focus term 2: an alternative term available for a subset of studies. May offer a better match for the lowest level of the standardized terminology.\n",
    "\n",
    "DL2 = 'PT_NAME' # Secondary level terms (i.e. more general than DL1 terms)\n",
    "DL3 = 'HLT_NAME' # Tertiary level terms (i.e. more general than DL2 terms)\n",
    "DL4 = 'HLGT_NAME' # Quarternary level terms (i.e. more general than DL3 terms)\n",
    "DL5 = 'SOC_NAME' # Quinary Level terms (i.e. more general than DL4 terms)\n",
    "\n",
    "TL1_qual_code_header = 'LLT_map_code' # column name for lowest term level mapping quality code (added to mapping file)\n",
    "TL1_name_header = 'T_LLT' # column name for the 'mapped' TL1 - term name (added to mapping file)\n",
    "TL1_code_header = 'T_LLT_CODE' # column name for the 'mapped' TL1 - term code (added to mapping file)\n",
    "TL2_name_header = 'T_PT'\n",
    "TL2_code_header = 'T_PT_CODE'\n",
    "TL3_name_header = 'T_HLT'\n",
    "TL3_code_header = 'T_HLT_CODE'\n",
    "TL4_name_header = 'T_HLGT'\n",
    "TL4_code_header = 'T_HLGT_CODE'\n",
    "TL5_name_header = 'T_SOC'\n",
    "TL5_code_header = 'T_SOC_CODE'\n",
    "\n",
    "FZ1_FT1 = 'FZMatch_1_'+DL1_FT1 # column name for best FT1 fuzzy match (temporarily added to mapping file)\n",
    "FZ2_FT1 = 'FZMatch_2_'+DL1_FT1 # column name for second best FT1 fuzzy match (temporarily added to mapping file)\n",
    "FZ3_FT1 = 'FZMatch_3_'+DL1_FT1 # column name for third best FT1 fuzzy match (temporarily added to mapping file)\n",
    "FZ4_FT1 = 'FZMatch_4_'+DL1_FT1 # column name for fourth best FT1 fuzzy match (temporarily added to mapping file)\n",
    "FZ5_FT1 = 'FZMatch_5_'+DL1_FT1 # column name for fifth best FT1 fuzzy match (temporarily added to mapping file)\n",
    "\n",
    "FZMC = 'FZMatch_Choice_ID_'+DL1_FT1 #column name for the column where manual annotator will enter the number (1-5) indicating the FT1 fuzzy matched term that offers the best match (if a good one is identified)\n",
    "FZCT = 'FZMatch_Copied_Term' #column name for the column where manual annotator can alternatively manually copy in the MedDRA LLT term that best matches the term information in this row (can come from FT2 or FT3 if term was not identified in FT1)\n",
    "\n",
    "FZ1_FT2 = 'FZMatch_1_'+DL1_FT2 # column name for best FT2 fuzzy match (temporarily added to mapping file)\n",
    "FZ2_FT2 = 'FZMatch_2_'+DL1_FT2 # column name for second best FT2 fuzzy match (temporarily added to mapping file)\n",
    "FZ3_FT2 = 'FZMatch_3_'+DL1_FT2 # column name for third best FT2 fuzzy match (temporarily added to mapping file)\n",
    "FZ4_FT2 = 'FZMatch_4_'+DL1_FT2 # column name for forth best FT2 fuzzy match (temporarily added to mapping file)\n",
    "FZ5_FT2 = 'FZMatch_5_'+DL1_FT2 # column name for fifth best FT2 fuzzy match (temporarily added to mapping file)\n",
    "\n",
    "FZ1_FT3 = 'FZMatch_1_'+DL1_FT3 # column name for best FT3 fuzzy match (temporarily added to mapping file)\n",
    "FZ2_FT3 = 'FZMatch_2_'+DL1_FT3 # column name for second best FT3 fuzzy match (temporarily added to mapping file)\n",
    "FZ3_FT3 = 'FZMatch_3_'+DL1_FT3 # column name for third best FT3 fuzzy match (temporarily added to mapping file)\n",
    "FZ4_FT3 = 'FZMatch_4_'+DL1_FT3 # column name for forth best FT3 fuzzy match (temporarily added to mapping file)\n",
    "FZ5_FT3 = 'FZMatch_5_'+DL1_FT3 # column name for fifth best FT3 fuzzy match (temporarily added to mapping file)\n",
    "\n",
    "TL3_B = 'HLT_branches'\n",
    "TL3_BQ = 'HLT_branch_quality'\n",
    "TL3_FZT = 'HLT_Fuzzy_Terms'\n",
    "TL3_FZS = 'HLT_Fuzzy_Scores'\n",
    "\n",
    "TL4_B = 'HLGT_branches'\n",
    "TL4_BQ = 'HLGT_branch_quality'\n",
    "TL4_FZT = 'HLGT_Fuzzy_Terms'\n",
    "TL4_FZS = 'HLGT_Fuzzy_Scores'\n",
    "\n",
    "TL5_B = 'SOC_branches'\n",
    "TL5_BQ = 'SOC_branch_quality'\n",
    "TL5_FZT = 'SOC_Fuzzy_Terms'\n",
    "TL5_FZS = 'SOC_Fuzzy_Scores'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Quality Control and Summary for all 'unique' rows in the map file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28720, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28720 entries, 0 to 28719\n",
      "Data columns (total 33 columns):\n",
      "ROW_INDEX                   28720 non-null int64\n",
      "MHTERM                      28720 non-null object\n",
      "FZMatch_Choice_ID_MHTERM    4602 non-null float64\n",
      "FZMatch_Copied_Term         2483 non-null object\n",
      "LLT_map_code                28720 non-null int64\n",
      "LLT_NAME                    17839 non-null object\n",
      "MHMODIFY                    16937 non-null object\n",
      "PT_NAME                     14235 non-null object\n",
      "HLT_NAME                    15542 non-null object\n",
      "HLGT_NAME                   15542 non-null object\n",
      "SOC_NAME                    25361 non-null object\n",
      "T_LLT                       28690 non-null object\n",
      "T_LLT_CODE                  28690 non-null float64\n",
      "T_PT                        28690 non-null object\n",
      "T_PT_CODE                   28690 non-null float64\n",
      "T_HLT                       28690 non-null object\n",
      "T_HLT_CODE                  28690 non-null float64\n",
      "HLT_branches                10090 non-null object\n",
      "HLT_branch_quality          28690 non-null float64\n",
      "HLT_Fuzzy_Terms             658 non-null object\n",
      "HLT_Fuzzy_Scores            658 non-null object\n",
      "T_HLGT                      28690 non-null object\n",
      "T_HLGT_CODE                 28690 non-null float64\n",
      "HLGT_branches               366 non-null object\n",
      "HLGT_branch_quality         28690 non-null float64\n",
      "HLGT_Fuzzy_Terms            3 non-null object\n",
      "HLGT_Fuzzy_Scores           3 non-null object\n",
      "T_SOC                       28690 non-null object\n",
      "T_SOC_CODE                  28690 non-null float64\n",
      "SOC_branches                515 non-null object\n",
      "SOC_branch_quality          28690 non-null float64\n",
      "SOC_Fuzzy_Terms             100 non-null object\n",
      "SOC_Fuzzy_Scores            100 non-null object\n",
      "dtypes: float64(9), int64(2), object(22)\n",
      "memory usage: 7.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROW_INDEX                   28720\n",
       "MHTERM                      21451\n",
       "FZMatch_Choice_ID_MHTERM        5\n",
       "FZMatch_Copied_Term          1130\n",
       "LLT_map_code                    7\n",
       "LLT_NAME                     6386\n",
       "MHMODIFY                    10482\n",
       "PT_NAME                      2662\n",
       "HLT_NAME                     1520\n",
       "HLGT_NAME                     529\n",
       "SOC_NAME                      139\n",
       "T_LLT                        6250\n",
       "T_LLT_CODE                   6229\n",
       "T_PT                         3264\n",
       "T_PT_CODE                    3264\n",
       "T_HLT                        1092\n",
       "T_HLT_CODE                   1092\n",
       "HLT_branches                  929\n",
       "HLT_branch_quality              5\n",
       "HLT_Fuzzy_Terms               123\n",
       "HLT_Fuzzy_Scores              108\n",
       "T_HLGT                        313\n",
       "T_HLGT_CODE                   313\n",
       "HLGT_branches                   8\n",
       "HLGT_branch_quality             5\n",
       "HLGT_Fuzzy_Terms                2\n",
       "HLGT_Fuzzy_Scores               2\n",
       "T_SOC                          27\n",
       "T_SOC_CODE                     27\n",
       "SOC_branches                    7\n",
       "SOC_branch_quality              5\n",
       "SOC_Fuzzy_Terms                 8\n",
       "SOC_Fuzzy_Scores               15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = pd.read_csv(\"MH_harmonization_map_22_TL5.csv\", na_values=' ') \n",
    "fd.shape\n",
    "fd.info()\n",
    "fd.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Check that terms and codes have been filled in for all rows that could be mapped (i.e. LLT mapping quality < 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb3bd343bff4bd9b67a677c4e058cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=28720, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Mapped Terms - Quality Report:\n",
      "LLT Term: 30, LLT Code: 30\n",
      "PT Term: 30, PT Code: 30\n",
      "HLT Term: 30, HLT Code: 30\n",
      "HLGT Term: 30, HLGT Code: 30\n",
      "SOC Term: 30, SOC Code: 30\n"
     ]
    }
   ],
   "source": [
    "quality = 0\n",
    "c_llt = 0\n",
    "c_llt_code = 0\n",
    "c_pt = 0\n",
    "c_pt_code = 0\n",
    "c_hlt = 0\n",
    "c_hlt_code = 0\n",
    "c_hlgt = 0\n",
    "c_hlgt_code = 0\n",
    "c_soc = 0\n",
    "c_soc_code = 0\n",
    "\n",
    "count = 0\n",
    "for each in tqdm_notebook(fd[TL1_name_header], desc='1st loop'): #for each row (i.e. AE term to be mapped)\n",
    "    if pd.isna(each): # Code missing    \n",
    "        c_llt += 1\n",
    "    if pd.isna(fd[TL1_code_header][count]):\n",
    "        c_llt_code += 1\n",
    "    if pd.isna(fd[TL1_qual_code_header][count]):\n",
    "        quality += 1\n",
    "        \n",
    "    if pd.isna(fd[TL2_name_header][count]):\n",
    "        c_pt += 1\n",
    "    if pd.isna(fd[TL2_code_header][count]):\n",
    "        c_pt_code += 1 \n",
    "        \n",
    "    if pd.isna(fd[TL3_name_header][count]):\n",
    "        c_hlt += 1\n",
    "    if pd.isna(fd[TL3_code_header][count]):\n",
    "        c_hlt_code += 1  \n",
    "        \n",
    "    if pd.isna(fd[TL4_name_header][count]):\n",
    "        c_hlgt += 1\n",
    "    if pd.isna(fd[TL4_code_header][count]):\n",
    "        c_hlgt_code += 1  \n",
    "        \n",
    "    if pd.isna(fd[TL5_name_header][count]):\n",
    "        c_soc += 1\n",
    "    if pd.isna(fd[TL5_code_header][count]):\n",
    "        c_soc_code += 1  \n",
    "    \n",
    "    count += 1\n",
    "        \n",
    "print('Missing Mapped Terms - Quality Report:')\n",
    "print('LLT Term: '+str(c_llt)+', LLT Code: '+str(c_llt_code))\n",
    "print('PT Term: '+str(c_pt)+', PT Code: '+str(c_pt_code))\n",
    "print('HLT Term: '+str(c_hlt)+', HLT Code: '+str(c_hlt_code))\n",
    "print('HLGT Term: '+str(c_hlgt)+', HLGT Code: '+str(c_hlgt_code))\n",
    "print('SOC Term: '+str(c_soc)+', SOC Code: '+str(c_soc_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Application Notes: 30 terms could not be mapped over all unique rows, but we confirm that terms and codes have been filled in for all other rows.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Survey frequency of branches at each level of the ontology where term branches were possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLT Branch Counts:\n",
      "{'4': 236, '0': 18630, '3': 2205, '5': 176, '2': 7473}\n",
      "HLGT Branch Counts:\n",
      "{'0': 28354, '2': 366}\n",
      "SOC Branch Counts:\n",
      "{'0': 28205, '2': 515}\n"
     ]
    }
   ],
   "source": [
    "#Survey HLT\n",
    "branch_dict = {}\n",
    "for each in fd[TL3_B]:\n",
    "    digets = len(str(each))\n",
    "    codes = int(int(digets)/8)\n",
    "    code_digets = str(codes)\n",
    "\n",
    "    if code_digets in branch_dict:               \n",
    "        branch_dict[code_digets] += 1\n",
    "    else:\n",
    "        branch_dict[code_digets] = 1\n",
    "print(\"HLT Branch Counts:\")\n",
    "print(branch_dict)\n",
    "                      \n",
    "#Survey HLGT\n",
    "branch_dict = {}\n",
    "for each in fd[TL4_B]:\n",
    "    digets = len(str(each))\n",
    "    codes = int(int(digets)/8)\n",
    "    code_digets = str(codes)\n",
    "\n",
    "    \n",
    "    if code_digets in branch_dict:               \n",
    "        branch_dict[code_digets] += 1\n",
    "    else:\n",
    "        branch_dict[code_digets] = 1\n",
    "print(\"HLGT Branch Counts:\")\n",
    "print(branch_dict)\n",
    "\n",
    "\n",
    "#Survey SOC\n",
    "branch_dict = {}\n",
    "for each in fd[TL5_B]:\n",
    "    digets = len(str(each))\n",
    "    codes = int(int(digets)/8)\n",
    "    code_digets = str(codes)\n",
    "\n",
    "    \n",
    "    if code_digets in branch_dict:               \n",
    "        branch_dict[code_digets] += 1\n",
    "    else:\n",
    "        branch_dict[code_digets] = 1\n",
    "print(\"SOC Branch Counts:\")\n",
    "print(branch_dict)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Application notes: recall that code 0 means there was no branch (i.e. direct term imputation), code 1 means that an exact match resolved the best branch, code 2 means that the branch with the highest fuzzy match score was ultimately selected, code 3 means that a different branch was selected manually, and code 4 means that no term information was available to perform exact or fuzzy branch term matching, thus the first branch was selected by default.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Survey LLT annotation quality code frequencies (as these are most important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE final annotation code counts:\n",
      "{'0': 3922, '3': 1136, '5': 14, '6': 29, '2': 13357, '1': 3190, '4': 7072}\n"
     ]
    }
   ],
   "source": [
    "annote_dict = {}\n",
    "for each in fd[TL1_qual_code_header]:\n",
    "    code = str(each)\n",
    "    if code in annote_dict :               \n",
    "        annote_dict[code] += 1\n",
    "    else:\n",
    "        annote_dict[code] = 1\n",
    "\n",
    "\n",
    "print(\"AE final annotation code counts:\")\n",
    "print(annote_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Application notes: Recall that codes 0-3 represent rows resolved with exact matching, codes 4 and 5 represent rows resolved by fuzzy matching, and code 6 indicates rows that could not be mapped.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Survey HLT Branch Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLT Branch code counts:\n",
      "{'1.0': 5013, '2.0': 347, '3.0': 311, '4.0': 4422, 'nan': 30, '0.0': 18597}\n"
     ]
    }
   ],
   "source": [
    "annote_dict = {}\n",
    "for each in fd[TL3_BQ]:\n",
    "    code = str(each)\n",
    "    if code in annote_dict :               \n",
    "        annote_dict[code] += 1\n",
    "    else:\n",
    "        annote_dict[code] = 1\n",
    "\n",
    "\n",
    "print(\"HLT Branch code counts:\")\n",
    "print(annote_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Application notes: Recall that code 0 indicates a direct term imputation, code 1 represents term branching resolved by exact matching, codes 2 and 3 represent term branching resolved by fuzzy matching, and code 4 represents branching where the first branch was picked by default (because not term information was available in the original dataset). No code was assigned here for any row where the original LLT term could not be mapped.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Survey HLGT Branch Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLGT Branch code counts:\n",
      "{'1.0': 237, '3.0': 2, '2.0': 1, '4.0': 126, 'nan': 30, '0.0': 28324}\n"
     ]
    }
   ],
   "source": [
    "annote_dict = {}\n",
    "for each in fd[TL4_BQ]:\n",
    "    code = str(each)\n",
    "    if code in annote_dict :               \n",
    "        annote_dict[code] += 1\n",
    "    else:\n",
    "        annote_dict[code] = 1\n",
    "\n",
    "\n",
    "print(\"HLGT Branch code counts:\")\n",
    "print(annote_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Application notes: Recall that code 0 indicates a direct term imputation, code 1 represents term branching resolved by exact matching, codes 2 and 3 represent term branching resolved by fuzzy matching, and code 4 represents branching where the first branch was picked by default (because not term information was available in the original dataset). No code was assigned here for any row where the original LLT term could not be mapped.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Survey SOC Branch Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC Branch code counts:\n",
      "{'1.0': 315, '2.0': 96, '3.0': 4, '4.0': 100, 'nan': 30, '0.0': 28175}\n"
     ]
    }
   ],
   "source": [
    "annote_dict = {}\n",
    "for each in fd[TL5_BQ]:\n",
    "    code = str(each)\n",
    "    if code in annote_dict :               \n",
    "        annote_dict[code] += 1\n",
    "    else:\n",
    "        annote_dict[code] = 1\n",
    "\n",
    "\n",
    "print(\"SOC Branch code counts:\")\n",
    "print(annote_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Application notes: Recall that code 0 indicates a direct term imputation, code 1 represents term branching resolved by exact matching, codes 2 and 3 represent term branching resolved by fuzzy matching, and code 4 represents branching where the first branch was picked by default (because not term information was available in the original dataset). No code was assigned here for any row where the original LLT term could not be mapped.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Map 'unique' rows back to original target dataset from first notebook\n",
    "- load original data and new mapped data\n",
    "- drop row index\n",
    "- add new relevant mapping columns to original dataset - initialize blank\n",
    "- orginal data is new working data - for each row see mapped data, check all relevant columns to confirm row-wide match\n",
    "- when match found copy the mapping data to the original data file. \n",
    "- deal with any original data rows that fail to match/map. \n",
    "- rerun QC analysis for this new dataset.  Include summary of numbers highlighting counts for unique rows vs all rows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37105, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37105 entries, 0 to 37104\n",
      "Data columns (total 15 columns):\n",
      "STUDY        37105 non-null object\n",
      "MHTERM       37083 non-null object\n",
      "MHCODE       5811 non-null float64\n",
      "MHMODIFY     19140 non-null object\n",
      "LLT_CODE     16348 non-null float64\n",
      "LLT_NAME     21523 non-null object\n",
      "PT_CODE      14114 non-null float64\n",
      "PT_NAME      19447 non-null object\n",
      "HLT_CODE     9597 non-null float64\n",
      "HLT_NAME     20105 non-null object\n",
      "HLGTCODE     9597 non-null float64\n",
      "HLGT_NAME    20105 non-null object\n",
      "SOC_CODE     14114 non-null float64\n",
      "SOC_NAME     33735 non-null object\n",
      "PTSOC_CD     3099 non-null float64\n",
      "dtypes: float64(7), object(8)\n",
      "memory usage: 4.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "STUDY           20\n",
       "MHTERM       21452\n",
       "MHCODE        2571\n",
       "MHMODIFY     10482\n",
       "LLT_CODE      4004\n",
       "LLT_NAME      6386\n",
       "PT_CODE       2414\n",
       "PT_NAME       2662\n",
       "HLT_CODE       756\n",
       "HLT_NAME      1520\n",
       "HLGTCODE       266\n",
       "HLGT_NAME      529\n",
       "SOC_CODE        26\n",
       "SOC_NAME       139\n",
       "PTSOC_CD        26\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = pd.read_excel(\"Combined_MEDHX_TERMS_20studies.xlsx\", sep='\\t',na_values=' ') #Data loaded so that blank excell cells are 'NA'\n",
    "td.shape\n",
    "td.info()\n",
    "td.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Insert relevant mapping columns into original target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add columns from new mapped file to the original dataset\n",
    "td.insert(loc=15,column=TL1_qual_code_header,value='') \n",
    "td.insert(loc=16,column=TL1_name_header,value='') \n",
    "td.insert(loc=17,column=TL1_code_header,value='') \n",
    "\n",
    "td.insert(loc=18,column=TL2_name_header,value='') \n",
    "td.insert(loc=19,column=TL2_code_header,value='') \n",
    "\n",
    "td.insert(loc=20,column=TL3_name_header,value='') \n",
    "td.insert(loc=21,column=TL3_code_header,value='') \n",
    "td.insert(loc=22,column=TL3_B,value='') \n",
    "td.insert(loc=23,column=TL3_BQ,value='') \n",
    "\n",
    "td.insert(loc=24,column=TL4_name_header,value='') \n",
    "td.insert(loc=25,column=TL4_code_header,value='') \n",
    "td.insert(loc=26,column=TL4_B,value='') \n",
    "td.insert(loc=27,column=TL4_BQ,value='') \n",
    "\n",
    "td.insert(loc=28,column=TL5_name_header,value='') \n",
    "td.insert(loc=29,column=TL5_code_header,value='') \n",
    "td.insert(loc=30,column=TL5_B,value='') \n",
    "td.insert(loc=31,column=TL5_BQ,value='') \n",
    "\n",
    "td.to_csv(\"MH_harmonization_map_23_testmap.csv\", header=True, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Fill in the values (for these inserted columns) from the working map file back to the original target dataset\n",
    "To find a matching row from the original target dataset we focus on the term information in DL1_FT1, DL1_FT2, and DL1_FT3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6122dfcc284ef18a81b23dab15d3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=37105, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n",
      "732\n",
      "1018\n",
      "1429\n",
      "1430\n",
      "1574\n",
      "1575\n",
      "2431\n",
      "2447\n",
      "2449\n",
      "2949\n",
      "2970\n",
      "3106\n",
      "3211\n",
      "3343\n",
      "4409\n",
      "5884\n",
      "6445\n",
      "6452\n",
      "11817\n",
      "11818\n",
      "11819\n",
      "11820\n",
      "11821\n",
      "11822\n",
      "11823\n",
      "11824\n",
      "11825\n",
      "11826\n",
      "11827\n",
      "11828\n",
      "14531\n",
      "14539\n",
      "15214\n",
      "20001\n",
      "21231\n",
      "22399\n",
      "22546\n",
      "23257\n",
      "23454\n",
      "23488\n",
      "23954\n",
      "24012\n",
      "24194\n",
      "24427\n",
      "25736\n",
      "27243\n",
      "29168\n",
      "29200\n",
      "29335\n",
      "29410\n",
      "29853\n",
      "30122\n",
      "30353\n",
      "30380\n",
      "30393\n",
      "30460\n",
      "30580\n",
      "30681\n",
      "30704\n",
      "31356\n",
      "31410\n",
      "31508\n",
      "31594\n",
      "31610\n",
      "31669\n",
      "31694\n",
      "31768\n",
      "34291\n",
      "37104\n"
     ]
    }
   ],
   "source": [
    "target_row = 0\n",
    "for each in tqdm_notebook(td[DL1_FT1], desc='1st loop'): #for each row in main data\n",
    "    #make list of target column terms\n",
    "    targetList = [each,td[DL1_FT2][target_row],td[DL1_FT3][target_row]]\n",
    "    other_row = 0\n",
    "    matched = False\n",
    "    for other in fd[DL1_FT1]:\n",
    "        #make list of other column terms\n",
    "        otherList = [other,fd[DL1_FT2][other_row],fd[DL1_FT3][other_row]]\n",
    "        \n",
    "        if targetList == otherList: #check for row match\n",
    "            \n",
    "            #Copy all relevant mapping column info to target dataset.\n",
    "            td[TL1_qual_code_header][target_row] = fd[TL1_qual_code_header][other_row]\n",
    "            td[TL1_name_header][target_row] = fd[TL1_name_header][other_row]\n",
    "            td[TL1_code_header][target_row] = fd[TL1_code_header][other_row]\n",
    "            \n",
    "            td[TL2_name_header][target_row] = fd[TL2_name_header][other_row]\n",
    "            td[TL2_code_header][target_row] = fd[TL2_code_header][other_row]\n",
    "            \n",
    "            td[TL3_name_header][target_row] = fd[TL3_name_header][other_row]\n",
    "            td[TL3_code_header][target_row] = fd[TL3_code_header][other_row]\n",
    "            td[TL3_B][target_row] = fd[TL3_B][other_row]\n",
    "            td[TL3_BQ][target_row] = fd[TL3_BQ][other_row]\n",
    "            \n",
    "            td[TL4_name_header][target_row] = fd[TL4_name_header][other_row]\n",
    "            td[TL4_code_header][target_row] = fd[TL4_code_header][other_row]\n",
    "            td[TL4_B][target_row] = fd[TL4_B][other_row]\n",
    "            td[TL4_BQ][target_row] = fd[TL4_BQ][other_row]\n",
    "            \n",
    "            td[TL5_name_header][target_row] = fd[TL5_name_header][other_row]\n",
    "            td[TL5_code_header][target_row] = fd[TL5_code_header][other_row]\n",
    "            td[TL5_B][target_row] = fd[TL5_B][other_row]\n",
    "            td[TL5_BQ][target_row] = fd[TL5_BQ][other_row]\n",
    "            \n",
    "            matched = True\n",
    "            break\n",
    "            \n",
    "        other_row += 1\n",
    "        \n",
    "    if not matched: \n",
    "        print(target_row)\n",
    "        \n",
    "    target_row += 1\n",
    "\n",
    "td.to_csv(\"MH_harmonization_map_23_fullmap.csv\", header=True, index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix up final mapped file\n",
    "- There are a number of reasons that mapping back to the original dataset will not be complete.  Here we go back and try to automate fixing up some of these issues. \n",
    "- For any row that has not been mapped or given a code of (6), i.e. unmappable, add a quality code of 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be6d2888982431cacdb165a38b02d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=37105, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "td = pd.read_csv(\"MH_harmonization_map_23_fullmap.csv\", na_values=' ') \n",
    "\n",
    "#Any TL1 that has no value, add a quality code of 6 (unmappable)\n",
    "target_row = 0\n",
    "for each in tqdm_notebook(td[DL1_FT1], desc='1st loop'): #for each row in main data\n",
    "    #make list of target column terms\n",
    "    if pd.isna(each): #Check for missing value\n",
    "        td[TL1_qual_code_header][target_row] = 6\n",
    "    target_row += 1\n",
    "    \n",
    "td.to_csv(\"MH_harmonization_map_24_fullmap.csv\", header=True, index=False) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual fixing of unmapped rows\n",
    "- First sorted the file by code and highlighted any rows with no mapped code\n",
    "- then sorted file by MHTerm - and used mapped rows to facilitate filling in entire set of mapping info (LLT, PT, etc) for as many unmapped rows as possible. \n",
    "- Some LLT's were added manually based on a separate search of MedDRA and will need to be imputed in a followup cleanup.\n",
    "\n",
    "Following these manual fixes we saved the file as (\"MH_harmonization_map_24_fullmap_manual.csv\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute hierarchy for any rows that have since added a mapped LLT that were missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary ontology files for imputing DL2 and DL3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78808, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(69531, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(23088, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1737, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(33402, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Load Lowest Level Terminology Standard File\n",
    "\n",
    "tl1 = pd.read_excel(ont_DL1_data , sep='\\t',na_values=' ')\n",
    "tl1.shape\n",
    "#Filter out any non-current low level terms (LLTs) \n",
    "tl1 = tl1.loc[tl1[ont_DL1_cur_col] == 'Y'] #column name is application specific.\n",
    "#Again determine number of remaining unique LLTs\n",
    "tl1.shape\n",
    "#Readjusts the row index values so there are no gaps in the sequence from the row removal (important for indexing later) \n",
    "tl1 = tl1.reset_index(drop=True) \n",
    "\n",
    "#### Load 2nd Level Terminology Standard File\n",
    "tl2 = pd.read_excel(ont_DL2_data , sep='\\t',na_values=' ')\n",
    "tl2.shape\n",
    "\n",
    "#### Load 3rd Level Terminology Standard File\n",
    "tl3 = pd.read_excel(ont_DL3_data , sep='\\t',na_values=' ')\n",
    "tl3.shape\n",
    "\n",
    "#### Load 2nd to 3rd Level Terminology Connection File\n",
    "tl3_tl2 = pd.read_excel(ont_DL3_DL2_data, sep='\\t',na_values=' ')\n",
    "tl3_tl2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in remaining TL1 codes, and directly impute the TL2 names and codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f26c2d450ff47f897a9ee59c9fbedd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=37105, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plastic surgery NOS\n",
      "Hyperpotassemia\n"
     ]
    }
   ],
   "source": [
    "#Impute term hierarchy for any remaining manually added LLT's\n",
    "td = pd.read_csv(\"MH_harmonization_map_24_fullmap_manual.csv\", na_values=' ') \n",
    "\n",
    "\n",
    "data_count = 0\n",
    "for each in tqdm_notebook(td[TL1_code_header], desc='1st loop'): #for each row \n",
    "    #if TL2 term missing and quality code is not 6(unmappable)\n",
    "    if pd.isna(td[TL2_name_header][data_count]) and str(td[TL1_qual_code_header][data_count]) != str(6):\n",
    "        #Idenfity the TL1 code in the MedDRA LLT file\n",
    "        tempList = tl1[ont_DL1_code_col].tolist() # the column name reference is specific to the loaded MedDRA file\n",
    "        tempIndex = tempList.index(each) \n",
    "\n",
    "        #Put the corresponding TL2 code into the key\n",
    "        TL2_code = tl1[ont_DL2_code_col][tempIndex] # the column name reference is specific to the loaded MedDRA file\n",
    "        td[TL2_code_header][data_count] = TL2_code\n",
    "\n",
    "        #Identify the TL2 name from the MedDRA PT file\n",
    "        tempList = tl2[ont_DL2_code_col].tolist() # the column name reference is specific to the loaded MedDRA file\n",
    "        tempIndex = tempList.index(TL2_code) \n",
    "\n",
    "        #Put the corresponding TL2 name into the key\n",
    "        TL2_name = tl2[ont_DL2_name_col][tempIndex] # the column name reference is specific to the loaded MedDRA file\n",
    "        td[TL2_name_header][data_count] = TL2_name\n",
    "        print(td[TL1_name_header][data_count])\n",
    "    data_count +=1 \n",
    "    \n",
    "td.to_csv(\"MH_harmonization_map_25_fullmap.csv\", header=True, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute remaining TL3 terms and codes\n",
    "In this case all the terms that needed to be imputed either had no branches, or no terms to allow for exact/fuzzy matching, thus those steps were not needed here (but could be added as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63589809ae84054b200deb129cb89fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=37105, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directly Imputed: 2\n",
      "With Branches: 0\n"
     ]
    }
   ],
   "source": [
    "td = pd.read_csv(\"MH_harmonization_map_25_fullmap.csv\", na_values=' ') \n",
    "\n",
    "data_count = 0\n",
    "total_branched = 0\n",
    "total_direct = 0\n",
    "\n",
    "for each in tqdm_notebook(td[TL2_code_header], desc='1st loop'): #for each row\n",
    "    #if TL3 term missing and quality code is not 6(unmappable)\n",
    "    if pd.isna(td[TL3_name_header][data_count]) and str(td[TL1_qual_code_header][data_count]) != str(6):\n",
    "        \n",
    "        #Idenfity the TL2 code in the MedDRA HLT_PT file\n",
    "        tempList = tl3_tl2[ont_DL2_code_col].tolist() # the column name reference is specific to the loaded MedDRA file\n",
    "        indexList = [i for i,val in enumerate(tempList) if val==each]\n",
    "\n",
    "        #Put all connected TL3 codes in TL3_B separated by an underscore\n",
    "        TL3_list = []\n",
    "        TL3_str = ''\n",
    "        for i in indexList:\n",
    "            TL3_list.append(tl3_tl2[ont_DL3_code_col][i]) # the column name reference is specific to the loaded MedDRA file\n",
    "            TL3_str += str(tl3_tl2[ont_DL3_code_col][i])+'_' # the column name reference is specific to the loaded MedDRA file\n",
    "\n",
    "        #Branch Reporting\n",
    "        if len(TL3_list) > 1: # is there more than one branch \n",
    "            total_branched += 1\n",
    "            td[TL3_B][data_count] = TL3_str\n",
    "\n",
    "        else: #only one branch found\n",
    "            total_direct += 1\n",
    "            #Identify term for single code\n",
    "            tempList = tl3[ont_DL3_code_col].tolist() # the column name reference is specific to the loaded MedDRA file\n",
    "\n",
    "            tempIndex = tempList.index(TL3_list[0]) #find index/location of code in tl3 set of terms\n",
    "            term = tl3[ont_DL3_name_col][tempIndex] # the column name reference is specific to the loaded MedDRA file\n",
    "\n",
    "            td[TL3_name_header][data_count] = term\n",
    "            td[TL3_code_header][data_count] = TL3_list[0]\n",
    "            td[TL3_BQ][data_count] = 0 # Top Branch Quality\n",
    "\n",
    "    data_count +=1 \n",
    "\n",
    "print(\"Directly Imputed: \" +str(total_direct))\n",
    "print(\"With Branches: \" +str(total_branched))\n",
    "\n",
    "td.to_csv(\"MH_harmonization_map_26_fullmap.csv\", header=True, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Application notes: In this case no branches were found on the terms we sought to impute, thus we don't need to attempt exact or fuzzy matching to pick the best branch.  However code may be added here to do so if needed.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary ontology files for imputing DL4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 9)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1755, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Load 4th Level Terminology Standard File\n",
    "tl4 = pd.read_excel(ont_DL4_data, sep='\\t',na_values=' ')\n",
    "tl4.shape\n",
    "\n",
    "#### Load 3th to 4th Level Terminology Connection File\n",
    "tl4_tl3 = pd.read_excel(ont_DL4_DL3_data, sep='\\t',na_values=' ')\n",
    "tl4_tl3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute remaining TL4 terms and codes\n",
    "In this case all the terms that needed to be imputed either had no branches, or no terms to allow for exact/fuzzy matching, thus those steps were not needed here (but could be added as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803f6fc693eb4b38b6bd595be64c1275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=37105, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directly Imputed: 2\n",
      "With Branches: 0\n"
     ]
    }
   ],
   "source": [
    "td = pd.read_csv(\"MH_harmonization_map_26_fullmap.csv\", na_values=' ') \n",
    "data_count = 0\n",
    "total_branched = 0\n",
    "total_direct = 0\n",
    "\n",
    "for each in tqdm_notebook(td[TL3_code_header], desc='1st loop'): #for each row\n",
    "    #if TL3 term missing and quality code is not 6(unmappable)\n",
    "    if pd.isna(td[TL4_name_header][data_count]) and str(td[TL1_qual_code_header][data_count]) != str(6):\n",
    "\n",
    "        #Idenfity the TL3 code in the MedDRA HLGT_HLT file\n",
    "        tempList = tl4_tl3[ont_DL3_code_col].tolist() # the column name reference is specific to the loaded MedDRA file\n",
    "        indexList = [i for i,val in enumerate(tempList) if val==int(float(each))]\n",
    " \n",
    "        #Put all connected TL4 codes in TL4_B separated by an underscore\n",
    "        TL4_list = []\n",
    "        TL4_str = ''\n",
    "        for i in indexList:\n",
    "            TL4_list.append(tl4_tl3[ont_DL4_code_col][i]) # the column name reference is specific to the loaded MedDRA file\n",
    "            TL4_str += str(tl4_tl3[ont_DL4_code_col][i])+'_' # the column name reference is specific to the loaded MedDRA file\n",
    "        \n",
    "        #Branch Reporting\n",
    "        if len(TL4_list) > 1: # is there more than one branch \n",
    "            total_branched += 1\n",
    "            td[TL4_B][data_count] = TL4_str\n",
    "\n",
    "        else: #only one branch found\n",
    "            total_direct += 1\n",
    "            #Identify term for single code\n",
    "            tempList = tl4[ont_DL4_code_col].tolist() # the column name reference is specific to the loaded MedDRA file\n",
    "\n",
    "            tempIndex = tempList.index(TL4_list[0]) #find index/location of code in tl4 set of terms\n",
    "            term = tl4[ont_DL4_name_col][tempIndex] # the column name reference is specific to the loaded MedDRA file\n",
    "\n",
    "            td[TL4_name_header][data_count] = term\n",
    "            td[TL4_code_header][data_count] = TL4_list[0]\n",
    "            td[TL4_BQ][data_count] = 0 # Top Branch Quality\n",
    "\n",
    "    data_count +=1 \n",
    "\n",
    "print(\"Directly Imputed: \" +str(total_direct))\n",
    "print(\"With Branches: \" +str(total_branched))\n",
    "\n",
    "td.to_csv(\"MH_harmonization_map_27_fullmap.csv\", header=True, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Application notes: In this case no branches were found on the terms we sought to impute, thus we don't need to attempt exact or fuzzy matching to pick the best branch.  However code may be added here to do so if needed.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary ontology files for imputing DL5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(354, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Load 5th Level Terminology Standard File\n",
    "tl5 = pd.read_excel(ont_DL5_data, sep='\\t',na_values=' ')\n",
    "tl5.shape\n",
    "\n",
    "#### Load 4th to 5th Level Terminology Connection File\n",
    "tl5_tl4 = pd.read_excel(ont_DL5_DL4_data, sep='\\t',na_values=' ')\n",
    "tl5_tl4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute remaining TL5 terms and codes\n",
    "In this case all the terms that needed to be imputed either had no branches, or no terms to allow for exact/fuzzy matching, thus those steps were not needed here (but could be added as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d844c33d7194092aecf1b96d237d283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=37105, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directly Imputed: 2\n",
      "With Branches: 0\n"
     ]
    }
   ],
   "source": [
    "td = pd.read_csv(\"MH_harmonization_map_27_fullmap.csv\", na_values=' ') \n",
    "data_count = 0\n",
    "total_branched = 0\n",
    "total_direct = 0\n",
    "\n",
    "for each in tqdm_notebook(td[TL4_code_header], desc='1st loop'): #for each row\n",
    "    #if TL3 term missing and quality code is not 6(unmappable)\n",
    "    if pd.isna(td[TL5_name_header][data_count]) and str(td[TL1_qual_code_header][data_count]) != str(6):\n",
    "\n",
    "        #Idenfity the TL4 code in the MedDRA HLT_PT file\n",
    "        tempList = tl5_tl4[ont_DL4_code_col].tolist() # the column name reference is specific to the loaded MedDRA file\n",
    "        indexList = [i for i,val in enumerate(tempList) if val==each]\n",
    "\n",
    "        #Put all connected TL5 codes in TL5_B separated by an underscore\n",
    "        TL5_list = []\n",
    "        TL5_str = ''\n",
    "        for i in indexList:\n",
    "            TL5_list.append(tl5_tl4[ont_DL5_code_col][i]) # the column name reference is specific to the loaded MedDRA file\n",
    "            TL5_str += str(tl5_tl4[ont_DL5_code_col][i])+'_' # the column name reference is specific to the loaded MedDRA file\n",
    "\n",
    "        #Branch Reporting\n",
    "        if len(TL5_list) > 1: # is there more than one branch \n",
    "            total_branched += 1\n",
    "            td[TL5_B][data_count] = TL5_str\n",
    "\n",
    "        else: #only one branch found\n",
    "            total_direct += 1\n",
    "            #Identify term for single code\n",
    "            tempList = tl5[ont_DL5_code_col].tolist() # the column name reference is specific to the loaded MedDRA file\n",
    "\n",
    "            tempIndex = tempList.index(TL5_list[0]) #find index/location of code in tl3 set of terms\n",
    "            term = tl5[ont_DL5_name_col][tempIndex] # the column name reference is specific to the loaded MedDRA file\n",
    "\n",
    "            td[TL5_name_header][data_count] = term\n",
    "            td[TL5_code_header][data_count] = TL5_list[0]\n",
    "            td[TL5_BQ][data_count] = 0 # Top Branch Quality\n",
    "\n",
    "    data_count +=1 \n",
    "\n",
    "print(\"Directly Imputed: \" +str(total_direct))\n",
    "print(\"With Branches: \" +str(total_branched))\n",
    "td.to_csv(\"MH_harmonization_map_28_fullmap.csv\", header=True, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Application notes: Performed some minor remainin manual cleanup of the file to fix some simple copy errors from previous manual fixes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Quality Control and Summary for all original rows in the target dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Check that terms and codes have been filled in for all rows that could be mapped (i.e. LLT mapping quality < 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33af6bbd52c4aad8b89737b7ab748de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=37105, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Mapped Terms - Quality Report:\n",
      "LLT Term: 84, LLT Code: 84\n",
      "PT Term: 84, PT Code: 84\n",
      "HLT Term: 84, HLT Code: 84\n",
      "HLGT Term: 84, HLGT Code: 84\n",
      "SOC Term: 84, SOC Code: 84\n"
     ]
    }
   ],
   "source": [
    "fd = pd.read_csv(\"MH_harmonization_map_28_fullmap_man.csv\", na_values=' ')\n",
    "\n",
    "quality = 0\n",
    "c_llt = 0\n",
    "c_llt_code = 0\n",
    "c_pt = 0\n",
    "c_pt_code = 0\n",
    "c_hlt = 0\n",
    "c_hlt_code = 0\n",
    "c_hlgt = 0\n",
    "c_hlgt_code = 0\n",
    "c_soc = 0\n",
    "c_soc_code = 0\n",
    "\n",
    "count = 0\n",
    "for each in tqdm_notebook(fd[TL1_name_header], desc='1st loop'): #for each row (i.e. AE term to be mapped)\n",
    "    if pd.isna(each): # Code missing    \n",
    "        c_llt += 1\n",
    "    if pd.isna(fd[TL1_code_header][count]):\n",
    "        c_llt_code += 1\n",
    "    if pd.isna(fd[TL1_qual_code_header][count]):\n",
    "        quality += 1\n",
    "        \n",
    "    if pd.isna(fd[TL2_name_header][count]):\n",
    "        c_pt += 1\n",
    "    if pd.isna(fd[TL2_code_header][count]):\n",
    "        c_pt_code += 1 \n",
    "        \n",
    "    if pd.isna(fd[TL3_name_header][count]):\n",
    "        c_hlt += 1\n",
    "    if pd.isna(fd[TL3_code_header][count]):\n",
    "        c_hlt_code += 1  \n",
    "        \n",
    "    if pd.isna(fd[TL4_name_header][count]):\n",
    "        c_hlgt += 1\n",
    "    if pd.isna(fd[TL4_code_header][count]):\n",
    "        c_hlgt_code += 1  \n",
    "        \n",
    "    if pd.isna(fd[TL5_name_header][count]):\n",
    "        c_soc += 1\n",
    "    if pd.isna(fd[TL5_code_header][count]):\n",
    "        c_soc_code += 1  \n",
    "    \n",
    "    count += 1\n",
    "        \n",
    "print('Missing Mapped Terms - Quality Report:')\n",
    "print('LLT Term: '+str(c_llt)+', LLT Code: '+str(c_llt_code))\n",
    "print('PT Term: '+str(c_pt)+', PT Code: '+str(c_pt_code))\n",
    "print('HLT Term: '+str(c_hlt)+', HLT Code: '+str(c_hlt_code))\n",
    "print('HLGT Term: '+str(c_hlgt)+', HLGT Code: '+str(c_hlgt_code))\n",
    "print('SOC Term: '+str(c_soc)+', SOC Code: '+str(c_soc_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Survey LLT annotation quality code frequencies (as these are most important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLT quality code counts:\n",
      "{'2': 15286, '1': 4934, '3': 2182, '6': 84, '0': 5375, '5': 34, '4': 9210}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37105"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annote_dict = {}\n",
    "for each in fd[TL1_qual_code_header]:\n",
    "    code = str(each)\n",
    "    if code in annote_dict :               \n",
    "        annote_dict[code] += 1\n",
    "    else:\n",
    "        annote_dict[code] = 1\n",
    "\n",
    "print(\"LLT quality code counts:\")\n",
    "print(annote_dict)\n",
    "len(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Survey frequency of branches at each level of the ontology where term branches were possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLT Branch Counts:\n",
      "{'0': 24248, '2': 9494, '3': 2849, '5': 222, '4': 292}\n",
      "HLGT Branch Counts:\n",
      "{'0': 36662, '2': 443}\n",
      "SOC Branch Counts:\n",
      "{'0': 36445, '2': 660}\n"
     ]
    }
   ],
   "source": [
    "#Survey HLT\n",
    "branch_dict = {}\n",
    "for each in fd[TL3_B]:\n",
    "    digets = len(str(each))\n",
    "    codes = int(int(digets)/8)\n",
    "    code_digets = str(codes)\n",
    "\n",
    "    if code_digets in branch_dict:               \n",
    "        branch_dict[code_digets] += 1\n",
    "    else:\n",
    "        branch_dict[code_digets] = 1\n",
    "print(\"HLT Branch Counts:\")\n",
    "print(branch_dict)\n",
    "                      \n",
    "#Survey HLGT\n",
    "branch_dict = {}\n",
    "for each in fd[TL4_B]:\n",
    "    digets = len(str(each))\n",
    "    codes = int(int(digets)/8)\n",
    "    code_digets = str(codes)\n",
    "\n",
    "    \n",
    "    if code_digets in branch_dict:               \n",
    "        branch_dict[code_digets] += 1\n",
    "    else:\n",
    "        branch_dict[code_digets] = 1\n",
    "print(\"HLGT Branch Counts:\")\n",
    "print(branch_dict)\n",
    "\n",
    "#Survey SOC\n",
    "branch_dict = {}\n",
    "for each in fd[TL5_B]:\n",
    "    digets = len(str(each))\n",
    "    codes = int(int(digets)/8)\n",
    "    code_digets = str(codes)\n",
    "\n",
    "    \n",
    "    if code_digets in branch_dict:               \n",
    "        branch_dict[code_digets] += 1\n",
    "    else:\n",
    "        branch_dict[code_digets] = 1\n",
    "print(\"SOC Branch Counts:\")\n",
    "print(branch_dict)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey HLT Branch Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLT Branch code counts:\n",
      "{'0.0': 24161, '1.0': 6571, '4.0': 5400, 'nan': 84, '3.0': 410, '2.0': 479}\n"
     ]
    }
   ],
   "source": [
    "annote_dict = {}\n",
    "for each in fd[TL3_BQ]:\n",
    "    code = str(each)\n",
    "    if code in annote_dict :               \n",
    "        annote_dict[code] += 1\n",
    "    else:\n",
    "        annote_dict[code] = 1\n",
    "\n",
    "print(\"HLT Branch code counts:\")\n",
    "print(annote_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey HLGT Branch Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLGT Branch code counts:\n",
      "{'0.0': 36578, '1.0': 284, '2.0': 1, 'nan': 84, '3.0': 4, '4.0': 154}\n"
     ]
    }
   ],
   "source": [
    "annote_dict = {}\n",
    "for each in fd[TL4_BQ]:\n",
    "    code = str(each)\n",
    "    if code in annote_dict :               \n",
    "        annote_dict[code] += 1\n",
    "    else:\n",
    "        annote_dict[code] = 1\n",
    "\n",
    "print(\"HLGT Branch code counts:\")\n",
    "print(annote_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey SOC Branch Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC Branch code counts:\n",
      "{'0.0': 36361, '1.0': 417, '4.0': 100, 'nan': 84, '3.0': 5, '2.0': 138}\n"
     ]
    }
   ],
   "source": [
    "annote_dict = {}\n",
    "for each in fd[TL5_BQ]:\n",
    "    code = str(each)\n",
    "    if code in annote_dict :               \n",
    "        annote_dict[code] += 1\n",
    "    else:\n",
    "        annote_dict[code] = 1\n",
    "\n",
    "print(\"SOC Branch code counts:\")\n",
    "print(annote_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook conclusions\n",
    "In this notebook we performed an initial summary of the working map file with only unique rows.  Then we mapped these unique rows back to the original target dataset that we started with in the first notebook, so that all columns in the original dataset are restored, but our new ontology standardized mapping columns are added. This notebook concludes by generating summary information over the original set of term rows regarding mapping quality scores, the possible branches during imputation, and the quality of branch resolution during imputation. \n",
    "\n",
    "This concludes this term harmonization analysis pipeline. We hope others fine it to be of use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
