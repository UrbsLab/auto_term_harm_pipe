{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Harmonization - STEP 1 : Data Preparation\n",
    "#### Author: Ryan Urbanowicz (ryanurb@upenn.edu) \n",
    "#### Institution: University of Pennsylvania - Perleman School of Medicine\n",
    "#### Project: CMREF Data Harmonization \n",
    "#### Date: 9/1/21\n",
    "#### Project Overview: \n",
    "This set of Jupyter notebooks have been set up to facilitate the process of term harminozation and make it as replicatable as possible. The ultimate goal is to take an existing set of term data from one or more sources that have been concatenated into a single dataset, and map the available terms to an ontological standard. This proceedure is aimed at resolving one term data type at a time, e.g. medical history terms, or adverse event terms. The output of this proceedure is copy of the original concatenated datasets that adds columns for the harmonized standards terms (at their most specific level), along with any levels of a generalized hierarchy of terms that may be useful for downstream data analysis. Further we add columns that track the quality of the mapping for each row and at each level of term specificity. \n",
    "\n",
    "In these notebooks we lay out and document the aspects of this project that are automated (and thus fully replicatable) as well as describe and provide instructions for those aspects that required manual subjective decision making (thus are not completely replicatable). These notebooks have been set up to be as generalizable to other term harmonization tasks as possible, however throughout these notebooks we will focus (as our target application) the task outlined below.  As a result it is expected that for any outside party to utilize these notebooks for their own harmonization tasks, some modification/customization of these notebooks will be required. Thus it is best to view these notebooks as a guide/template for future term harmonization tasks, as well as a record for reproducing the harmonization tasks that we completed for our own target application. Additionally some of the computational tasks can take a number of hours to complete, therefore 'in-progress' mapping files in .csv and excel format are saved and loaded by this code after most steps. \n",
    "\n",
    "In general the materials required to utilize this notebook as a guide is (1) a set of source terms as rows in a dataset (including supplemental term columns, and/or any available hierarchy of more general terms from a relevant ontolology) and (2)  the reference files of a target ontology standard, e.g. MedDRA.  While these notebooks are designed specifically to utilize the MedDRA v21 ontology of terms, they could be adapted to any ontology that provides as standard terminology, as well as link between specific and more general terms as part of the ontology hierarchy (e.g. GO terms/hierarchy). \n",
    "\n",
    "#### Target Application:\n",
    "The specific 'term harmonization task' that we tackled in building these notebooks, is to harmonize terms used to describe 'medical history' events (MHTERM) for subjects over a set of 28 separate drug trials (CMREF Project). We will use the MedDRA v21 heirarchy of terms as our ontological/terminology standard. We labor under the assumption that not all drug trials used the MedDRA standard, and those that did likely did not used the same version (v21). The primary goal is to harmonize the available specific 'medical history' term information, such that all possible terms are mapped to MedDRA low level terms (LLT). The secondary goal is to impute values for the more general levels of the MedDRA term hierarchy (i.e. preferred terms (PT), higher level term (HLT), higher level group term (HLGT), and system organ class (SOC)). In our application all trials have terms available at a specific, 'patient reported' level which we focus on in the primary LLT mapping. However in some trials, supplemental term data is available beyond the patient reported terms that we will utilize when available to provide a greater opportunity to map these term rows to the LLT MedDRA standard. <br>\n",
    "\n",
    "#### Data Availability:\n",
    "While the target term data used in this study has not been made available here (for privacy and proprietary reasons) we have included the MedDRA ontology files that we formatted as Excel files. \n",
    "\n",
    "#### Code Generalization: \n",
    "In order to keep the code as generalizable (to other projects/applications) as possible we describe below the labels used in the code herein and how it correponds to our specific target application described above:\n",
    "\n",
    "*Format (General Code Labels) = (Application Specific Labels), i.e. column header names from our target dataset)*\n",
    "\n",
    "Ontology Standard:\n",
    "* (TL1) - Term Level 1 = (LLT) - lower level term\n",
    "* (TL2) - Term Level 2 = (PT) - preferred term\n",
    "* (TL3) - Term Level 3 = (HLT) - higher level term\n",
    "* (TL4) - Term Level 4 = (HLGT) - higher level group term\n",
    "* (TL5) - Term Level 5 = (SOC) - system organ class term\n",
    "\n",
    "Target Dataset:\n",
    "* (DL1_FT1) - Data Level 1 Focus Term 1  = (MHTERM) - Medical History Term (assumed available for every row)\n",
    "* (DL1_FT2) - Data Level 1 Focus Term 2  = (LLT_NAME) - The primary alternative term.  In this case the original attemped mapping to LLT. (availble for some rows)\n",
    "* (DL1_FT3) - Data Level 1 Focus Term 3  = (MHMODIFY) - An alternative, 'modified' term available in the data for mapping to the lowest term level . (available for some rows)\n",
    "* (DL2) - Data Level 2 = (PT_NAME) - MedDRA (unknown version) preferred term (available for some rows)\n",
    "* (DL3) - Data Level 3 = (HLT_NAME) - MedDRA (unknown version) higher level term (available for some rows)\n",
    "* (DL4) - Data Level 4 = (HLGT_NAME) - MedDRA (unknown version) higher level group term (available for some rows)\n",
    "* (DL5) - Data Level 5 = (SOC_NAME) - MedDRA (unknown version) system organ class term (available for some rows)\n",
    "\n",
    "#### Notebook Summary:\n",
    "This notebook is meant to cover initial data preparation.  This part of the harmonization process may include (1) loading the data, (2) summarizing the data for review, (3) cleaning the data (as needed), (4) formatting the data (as needed for running the remainder of the pipeline, (5) saving the cleaned, formatted file for the remainder of the harmonization pipeline. The target data is the concatenation of all available term instance rows over all studies to be harmonized.  This notebook also loads and provides summary information on the relevant MedDRA ontology files that will serve as our standard . \n",
    "\n",
    "#### Dependencies:\n",
    "We recommend users have Python 3 or higher, having installed the anaconda python package (e.g. https://www.datacamp.com/community/tutorials/installing-anaconda-windows). It may be necessary to install some additional packages that are used by one or more of these notebooks.  We recommend using the 'pip install' mechanism for easily installing these packages (e.g. https://packaging.python.org/tutorials/installing-packages/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python packages required in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#Load necessary packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Jupyter Notebook Hack: This code ensures that the results of multiple commands within a given cell are all displayed, rather than just the last. \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Load Target Data\n",
    "Our target dataset includes all individual medical history records for all subjects over all studies organized as rows. Columns are the variables as outlined above under \"target dataset\". Some terms in our intial dataset are not useful for the harmonization process so they will be excluded upfront. This includes \"STUDY\", the study identifier for our target application, which can be dropped here as it plays no role in the term mapping. There are also columns for old MedDRA codes for each of the 5 term levels in the MedDRA term hierarchy (we cannot confirm which MedDRA version these terms came from so they will be ignored).  These will be replaced with MedDRA v21 term codes during the mapping/imputation process. \n",
    "\n",
    "Below we begin by assigning the target-data-specific file and column names to general variables to facilitate adaptaion of this notebook to a different target problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create general variable names for the target-application-specific names needed across all project notebooks\n",
    "Upfront we create generally named variables to store all application-specific names needed across all notebooks for this project. Not all of these variables are needed within each notebook, but for ease of notebook adaptation we will specify them all upfront here, and load this cell once towards the beginning of each notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input filename for 'target dataset' (excel file loaded in this application)\n",
    "target_study_data = 'Combined_MEDHX_TERMS_20studies.xlsx' \n",
    "\n",
    "ont_DL1_data = 'LLT.xlsx' # Input filename for ontology file defining all DL1 terms and their codes. \n",
    "ont_DL1_name_col = 'llt_name' # column label for DL1 term name\n",
    "ont_DL1_code_col ='llt_code' # column label for DL2 term code\n",
    "ont_DL1_cur_col = 'llt_currency' # column label for term currency\n",
    "ont_DL2_data = 'PT.xlsx' # Input filename for ontology file defining all DL2 terms and their codes. \n",
    "ont_DL3_DL2_data = 'HLT_PT.xlsx' # Input filename for ontology file defining connections between DL2 and DL3 term codes. \n",
    "ont_DL3_data = 'HLT.xlsx' # Input filename for ontology file defining all DL3 terms and their codes.\n",
    "ont_DL4_DL3_data = 'HLGT_HLT.xlsx' # Input filename for ontology file defining connections between DL3 and DL4 term codes. \n",
    "ont_DL4_data = 'HLGT.xlsx' # Input filename for ontology file defining all DL4 terms and their codes.\n",
    "ont_DL5_DL4_data = 'SOC_HLGT.xlsx' # Input filename for ontology file defining connections between DL4 and DL5 term codes. \n",
    "ont_DL5_data = 'SOC.xlsx' # Input filename for ontology file defining all DL5 terms and their codes.\n",
    "\n",
    "DL1_FT1 = 'MHTERM' # focus term 1: This term is available over all studies. \n",
    "DL1_FT2 = 'LLT_NAME' # focus term 3: an alternative term available for a subset of studies. This one supposedly conforms to the MedDRA standard so we expect it to yield more exact matches. May offer a better match for the lowest level of the standardized terminology.\n",
    "DL1_FT3 = 'MHMODIFY' # focus term 2: an alternative term available for a subset of studies. May offer a better match for the lowest level of the standardized terminology.\n",
    "\n",
    "DL2 = 'PT_NAME' # Secondary level terms (i.e. more general than DL1 terms)\n",
    "DL3 = 'HLT_NAME' # Tertiary level terms (i.e. more general than DL2 terms)\n",
    "DL4 = 'HLGT_NAME' # Quarternary level terms (i.e. more general than DL3 terms)\n",
    "DL5 = 'SOC_NAME' # Quinary Level terms (i.e. more general than DL4 terms)\n",
    "\n",
    "TL1_qual_code_header = 'LLT_map_code' # column name for lowest term level mapping quality code (added to mapping file)\n",
    "TL1_name_header = 'T_LLT' # column name for the 'mapped' TL1 - term name (added to mapping file)\n",
    "TL1_code_header = 'T_LLT_CODE' # column name for the 'mapped' TL1 - term code (added to mapping file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load target (tab-delimited) file into a pandas data frame\n",
    "td = pd.read_excel(target_study_data, sep='\\t',na_values=' ') #Data loaded so that blank excell cells are 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Summarize Target Data\n",
    "Use pandas functions to orient ourselves to fundamental characteristics of the target dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STUDY</th>\n",
       "      <th>MHTERM</th>\n",
       "      <th>MHCODE</th>\n",
       "      <th>MHMODIFY</th>\n",
       "      <th>LLT_CODE</th>\n",
       "      <th>LLT_NAME</th>\n",
       "      <th>PT_CODE</th>\n",
       "      <th>PT_NAME</th>\n",
       "      <th>HLT_CODE</th>\n",
       "      <th>HLT_NAME</th>\n",
       "      <th>HLGTCODE</th>\n",
       "      <th>HLGT_NAME</th>\n",
       "      <th>SOC_CODE</th>\n",
       "      <th>SOC_NAME</th>\n",
       "      <th>PTSOC_CD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amb112565</td>\n",
       "      <td>Right ventricular hypertrophy with strain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right ventricular hypertrophy with strain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amb112565</td>\n",
       "      <td>Systemic Scleroderma Stage 11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Systemic Scleroderma Stage 11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amb112565</td>\n",
       "      <td>T-wave negatve in II, III, aVF, V1-6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T-wave negatve in II, III, aVF, V1-6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amb112565</td>\n",
       "      <td>chronic pericardial effusion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chronic pericardial effusion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amb112565</td>\n",
       "      <td>dyspnea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dyspnea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       STUDY                                      MHTERM  MHCODE  \\\n",
       "0  amb112565   Right ventricular hypertrophy with strain     NaN   \n",
       "1  amb112565               Systemic Scleroderma Stage 11     NaN   \n",
       "2  amb112565        T-wave negatve in II, III, aVF, V1-6     NaN   \n",
       "3  amb112565                chronic pericardial effusion     NaN   \n",
       "4  amb112565                                     dyspnea     NaN   \n",
       "\n",
       "                                    MHMODIFY  LLT_CODE LLT_NAME  PT_CODE  \\\n",
       "0  Right ventricular hypertrophy with strain       NaN      NaN      NaN   \n",
       "1              Systemic Scleroderma Stage 11       NaN      NaN      NaN   \n",
       "2       T-wave negatve in II, III, aVF, V1-6       NaN      NaN      NaN   \n",
       "3               chronic pericardial effusion       NaN      NaN      NaN   \n",
       "4                                    dyspnea       NaN      NaN      NaN   \n",
       "\n",
       "  PT_NAME  HLT_CODE HLT_NAME  HLGTCODE HLGT_NAME  SOC_CODE SOC_NAME  PTSOC_CD  \n",
       "0     NaN       NaN      NaN       NaN       NaN       NaN      NaN       NaN  \n",
       "1     NaN       NaN      NaN       NaN       NaN       NaN      NaN       NaN  \n",
       "2     NaN       NaN      NaN       NaN       NaN       NaN      NaN       NaN  \n",
       "3     NaN       NaN      NaN       NaN       NaN       NaN      NaN       NaN  \n",
       "4     NaN       NaN      NaN       NaN       NaN       NaN      NaN       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the first few rows of the dataset\n",
    "td.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37105, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Report the number of (rows,columns)\n",
    "td.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37105 entries, 0 to 37104\n",
      "Data columns (total 15 columns):\n",
      "STUDY        37105 non-null object\n",
      "MHTERM       37083 non-null object\n",
      "MHCODE       5811 non-null float64\n",
      "MHMODIFY     19140 non-null object\n",
      "LLT_CODE     16348 non-null float64\n",
      "LLT_NAME     21523 non-null object\n",
      "PT_CODE      14114 non-null float64\n",
      "PT_NAME      19447 non-null object\n",
      "HLT_CODE     9597 non-null float64\n",
      "HLT_NAME     20105 non-null object\n",
      "HLGTCODE     9597 non-null float64\n",
      "HLGT_NAME    20105 non-null object\n",
      "SOC_CODE     14114 non-null float64\n",
      "SOC_NAME     33735 non-null object\n",
      "PTSOC_CD     3099 non-null float64\n",
      "dtypes: float64(7), object(8)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Reports the column labels, number of rows with values, and the variable type that pandas thinks is in the given column.\n",
    "td.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: As we can see, in this target application, the dataset includes 37105 rows, but only 37083 have a value for 'MHTERM' (i.e. our DL1_FT1). These rows will be dropped from consideration given that we assume all rows we want to map will have an entry for this column.  These are artifacts from how the target dataset was assembled.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STUDY           20\n",
       "MHTERM       21452\n",
       "MHCODE        2571\n",
       "MHMODIFY     10482\n",
       "LLT_CODE      4004\n",
       "LLT_NAME      6386\n",
       "PT_CODE       2414\n",
       "PT_NAME       2662\n",
       "HLT_CODE       756\n",
       "HLT_NAME      1520\n",
       "HLGTCODE       266\n",
       "HLGT_NAME      529\n",
       "SOC_CODE        26\n",
       "SOC_NAME       139\n",
       "PTSOC_CD        26\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Report the number of unique values in each column.\n",
    "td.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: As we can see, in this target application, the dataset includes 37105 rows, but only 21542 unique 'level 1' terms. There are only 20 studies with Medical History data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Clean Target Data\n",
    "(1) Select only the columns needed for the term mapping (2) Missing value assessment (3) reduce dataset down to just the set of non-redundant rows, i.e. rows that contain a unique set of term entries (4) any other application specific data cleaning. We reduce the dataset down to just the unique rows to reduce the computational time required for this mapping proceedure, as well as to reduce the burden of manual mapping that will be required downstream. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset of necessary columns\n",
    "This step is very specific to the target application. We have manually endered the column headers from the original dataset that we want to keep for the mapping process, and resulting mapping file. In particular we are only keeping the three 'level one' focus terms, and the columns including any available more general MedDRA term levels (i.e. DL2, DL3, DL4, DL5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimmensions before..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37105, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimmensions after..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37105, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select subset of dataset columns from the entire target dataset.\n",
    "print(\"Data dimmensions before..\")\n",
    "td.shape\n",
    "td = td[[DL1_FT1,DL1_FT2,DL1_FT3,DL2,DL3,DL4,DL5]] \n",
    "print(\"Data dimmensions after..\")\n",
    "td.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Assessment\n",
    "(1) Determine if any 'DL1_FT1' rows have missing values (the assumption is that all of these rows to be mapped will have a value for DL1_FT1, otherwise that row can be thrown out), (2) Remove any rows with missing focus terms, (3) adjust row index values (for propper row referencing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Level 1 terms available (Not missing)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37083"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Value Counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MHTERM          22\n",
       "LLT_NAME     15582\n",
       "MHMODIFY     17965\n",
       "PT_NAME      17658\n",
       "HLT_NAME     17000\n",
       "HLGT_NAME    17000\n",
       "SOC_NAME      3370\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm number of Level 1 terms - i.e. Level 1 rows without missing values\n",
    "print(\"Number of Level 1 terms available (Not missing)\")\n",
    "td[DL1_FT1].count()\n",
    "\n",
    "#Evaluate missingness and data availability\n",
    "print(\"Missing Value Counts\")\n",
    "td.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: Out of 37105 rows, 22 have a DL1_FT1 term that is 'missing', meaning that these rows need to be dropped.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of remaining rows/columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37083, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Value Counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MHTERM           0\n",
       "LLT_NAME     15560\n",
       "MHMODIFY     17943\n",
       "PT_NAME      17636\n",
       "HLT_NAME     16978\n",
       "HLGT_NAME    16978\n",
       "SOC_NAME      3360\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms/values within each term level\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MHTERM       21452\n",
       "LLT_NAME      6386\n",
       "MHMODIFY     10482\n",
       "PT_NAME       2662\n",
       "HLT_NAME      1520\n",
       "HLGT_NAME      529\n",
       "SOC_NAME       139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop rows with missing values in DL1_FT1\n",
    "td = td.dropna(subset=[DL1_FT1])\n",
    "print(\"Number of remaining rows/columns\")\n",
    "td.shape\n",
    "print(\"Missing Value Counts\")\n",
    "td.isnull().sum()\n",
    "print(\"Number of unique terms/values within each term level\")\n",
    "td.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: Noticed that many DL1_FT1 terms are redundant over the instances in the studies (21452 unique terms in total)* <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify set of non-redundant rows. \n",
    "To reduce the computational and manual burden of many of the term matching tasks ahead it makes the most sense to begin by reducing the dataset down to a non-redundant set of rows.  Redundancy is defined as a row having the exact same set of present or absent term values as those found in another row.  In this application that includes the values found in the columns: (DL1_FT1, DL1_FT2, DL1_FT3, DL2, DL3, DL4, DL5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = td.drop_duplicates(subset=None, keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Readjusts the row index values so there are no gaps in the sequence from the row removal (important for indexing later) \n",
    "td = td.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28720, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "MHTERM       21452\n",
       "LLT_NAME      6386\n",
       "MHMODIFY     10482\n",
       "PT_NAME       2662\n",
       "HLT_NAME      1520\n",
       "HLGT_NAME      529\n",
       "SOC_NAME       139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.shape\n",
    "#Report the number of unique values for each variable.\n",
    "td.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: After removing redundant rows we have confirmed that the number of unique terms for all columns is maintained, and we are left with a total of 28720 unique rows to be mapped.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Save formatted data\n",
    "Before moving on to the next step of the harmonization pipeline we will save our mapping file 'in progress'. We will save the file such that it includes a new column identifying the unique row index value for each row.  We have found this useful to include early on for quality control purposes, particularly since not all steps are automated, and later, participants in this harmonization process may find it useful to sort the file by different columns to facilitate the mapping process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this script MH = Medical History for our target application.\n",
    "td.to_csv(\"MH_harmonization_map_1.csv\", header=True, index=True, index_label='ROW_INDEX') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Examine Terminology Standard Files\n",
    "In this first notebook we also load and summarizing basic information on the different ontology files. In this particular application we are using MedDRA v21 as our terminology standard. We have previously prepared excel formatted files for the MedDRA term mapping that links terms within each level (i.e. LLT, PT, HLT, HLGT, SOC) to their respective codes, and separate files to link (1) PTs to more general HLTs, (2) HLTs to more general HLGTs, and (3) HLGTs to more general SOCs. The LLT file includes direct mappings to PTs.  Note that in this hierarchy of MedDRA terms, all LLTs directly map to a single PT, but PTs and subsequently more general levels can map to multiple more general terms.  In this pipeline we will refer to this as *branching*. Furthermore, note that these files include terms that are active in MedDRA v21, as well as those that are out of date. These are differentated with the variable 'llt_currency'. In this pipeline we will be sure to only use MedDRA terms that are included in the current version (i.e. llt_currency == Y). \n",
    "\n",
    "Recall the general labels we had previously assigned for the levels of this ontology above. \n",
    "\n",
    "* (TL1) - Term Level 1 = (LLT) - lower level term\n",
    "* (TL2) - Term Level 2 = (PT) - preferred term\n",
    "* (TL3) - Term Level 3 = (HLT) - higher level term\n",
    "* (TL4) - Term Level 4 = (HLGT) - higher level group term\n",
    "* (TL5) - Term Level 5 = (SOC) - system organ class term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Summarize Term Level 1 File (MedDRA LLT file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78808, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 78808 entries, 0 to 78807\n",
      "Data columns (total 11 columns):\n",
      "llt_code            78808 non-null int64\n",
      "llt_name            78808 non-null object\n",
      "pt_code             78808 non-null int64\n",
      "llt_whoart_code+    0 non-null float64\n",
      "llt_harts_code+     0 non-null float64\n",
      "llt_costart_sym+    0 non-null float64\n",
      "llt_icd9_code+      0 non-null float64\n",
      "llt_icd9cm_code+    0 non-null float64\n",
      "llt_icd10_code+     0 non-null float64\n",
      "llt_currency        78808 non-null object\n",
      "llt_jart_code+      0 non-null float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 6.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78808"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "llt_code            78808\n",
       "llt_name            78807\n",
       "pt_code             23088\n",
       "llt_whoart_code+        0\n",
       "llt_harts_code+         0\n",
       "llt_costart_sym+        0\n",
       "llt_icd9_code+          0\n",
       "llt_icd9cm_code+        0\n",
       "llt_icd10_code+         0\n",
       "llt_currency            2\n",
       "llt_jart_code+          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl1 = pd.read_excel(ont_DL1_data, sep='\\t',na_values=' ')\n",
    "tl1.shape\n",
    "tl1.info()\n",
    "#Count LLTs in MedDRA file - check for missing\n",
    "tl1[ont_DL1_name_col].count() #column name is application specific.\n",
    "#Count unique LLTs in MedDRA file\n",
    "tl1.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: Confirmed that there are no redundant MedDRA LLTs. We note that there is an addional Y/N variable, 'llt_currency' that indicates if the term is currently used in v21 of MedDRA.  To ensure our termiology is v21 compliant, we will perform our term matching using only LLTs that are current (i.e.  llt_currency = Y).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for 'current' LLT's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llt_code            69531\n",
       "llt_name            69531\n",
       "pt_code             23088\n",
       "llt_whoart_code+        0\n",
       "llt_harts_code+         0\n",
       "llt_costart_sym+        0\n",
       "llt_icd9_code+          0\n",
       "llt_icd9cm_code+        0\n",
       "llt_icd10_code+         0\n",
       "llt_currency            1\n",
       "llt_jart_code+          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(69531, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter out any non-current low level terms (LLTs) \n",
    "tl1 = tl1.loc[tl1[ont_DL1_cur_col] == 'Y'] #column name is application specific.\n",
    "#Again determine number of remaining unique LLTs\n",
    "tl1.nunique()\n",
    "tl1.shape\n",
    "#Readjusts the row index values so there are no gaps in the sequence from the row removal (important for indexing later) \n",
    "tl1 = tl1.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: A total of 69531 unique and current LLT MedDRA terms/codes observed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Summarize Term Level 2 Files (MedDRA PT files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23088, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23088 entries, 0 to 23087\n",
      "Data columns (total 11 columns):\n",
      "pt_code            23088 non-null int64\n",
      "pt_name            23088 non-null object\n",
      "null_field         0 non-null float64\n",
      "pt_soc_code        23088 non-null int64\n",
      "pt_whoart_code     0 non-null float64\n",
      "pt_harts_code      0 non-null float64\n",
      "pt_costart_sym     0 non-null float64\n",
      "pt_icd9_code+      0 non-null float64\n",
      "pt_icd9cm_code+    0 non-null float64\n",
      "pt_icd10_code+     0 non-null float64\n",
      "pt_jart_code+      0 non-null float64\n",
      "dtypes: float64(8), int64(2), object(1)\n",
      "memory usage: 1.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pt_code            23088\n",
       "pt_name            23088\n",
       "null_field             0\n",
       "pt_soc_code           27\n",
       "pt_whoart_code         0\n",
       "pt_harts_code          0\n",
       "pt_costart_sym         0\n",
       "pt_icd9_code+          0\n",
       "pt_icd9cm_code+        0\n",
       "pt_icd10_code+         0\n",
       "pt_jart_code+          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl2 = pd.read_excel(ont_DL2_data, sep='\\t',na_values=' ') #Data loaded so that blank excell cells are 'NA'\n",
    "tl2.shape\n",
    "tl2.info()\n",
    "tl2.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: A total of 23088 unique PT MedDRA terms/codes observed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Summarize Term Level 3 Files (MedDRA HLT files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33402, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33402 entries, 0 to 33401\n",
      "Data columns (total 2 columns):\n",
      "hlt_code    33402 non-null int64\n",
      "pt_code     33402 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 522.0 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hlt_code     1737\n",
       "pt_code     23088\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl3_tl2 = pd.read_excel(ont_DL3_DL2_data, sep='\\t',na_values=' ') #Data loaded so that blank excell cells are 'NA'\n",
    "tl3_tl2.shape\n",
    "tl3_tl2.info()\n",
    "tl3_tl2.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1737, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1737 entries, 0 to 1736\n",
      "Data columns (total 9 columns):\n",
      "hlt_code            1737 non-null int64\n",
      "hlt_name            1737 non-null object\n",
      "hlt_whoart_code+    0 non-null float64\n",
      "hlt_harts_code      0 non-null float64\n",
      "hlt_costart_sym+    0 non-null float64\n",
      "hlt_icd9_code+      0 non-null float64\n",
      "hlt_icd9cm_code+    0 non-null float64\n",
      "hlt_icd10_code+     0 non-null float64\n",
      "hlt_jart_code+      0 non-null float64\n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 122.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hlt_code            1737\n",
       "hlt_name            1737\n",
       "hlt_whoart_code+       0\n",
       "hlt_harts_code         0\n",
       "hlt_costart_sym+       0\n",
       "hlt_icd9_code+         0\n",
       "hlt_icd9cm_code+       0\n",
       "hlt_icd10_code+        0\n",
       "hlt_jart_code+         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl3 = pd.read_excel(ont_DL3_data, sep='\\t',na_values=' ') #Data loaded so that blank excell cells are 'NA'\n",
    "tl3.shape\n",
    "tl3.info()\n",
    "tl3.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: A total of 1737 unique HLT MedDRA terms/codes observed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Summarize Term Level 4 Files (MedDRA HLGT files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1755, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1755 entries, 0 to 1754\n",
      "Data columns (total 2 columns):\n",
      "hlgt_code    1755 non-null int64\n",
      "hlt_code     1755 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 27.5 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hlgt_code     337\n",
       "hlt_code     1737\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl4_tl3 = pd.read_excel(ont_DL4_DL3_data, sep='\\t',na_values=' ') #Data loaded so that blank excell cells are 'NA'\n",
    "tl4_tl3.shape\n",
    "tl4_tl3.info()\n",
    "tl4_tl3.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 337 entries, 0 to 336\n",
      "Data columns (total 9 columns):\n",
      "hlgt_code            337 non-null int64\n",
      "hlgt_name            337 non-null object\n",
      "hlgt_whoart_code+    0 non-null float64\n",
      "hlgt_harts_code      0 non-null float64\n",
      "hlgt_costart_sym+    0 non-null float64\n",
      "hlgt_icd9_code+      0 non-null float64\n",
      "hlgt_icd9cm_code+    0 non-null float64\n",
      "hlgt_icd10_code+     0 non-null float64\n",
      "hlgt_jart_code+      0 non-null float64\n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 23.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hlgt_code            337\n",
       "hlgt_name            337\n",
       "hlgt_whoart_code+      0\n",
       "hlgt_harts_code        0\n",
       "hlgt_costart_sym+      0\n",
       "hlgt_icd9_code+        0\n",
       "hlgt_icd9cm_code+      0\n",
       "hlgt_icd10_code+       0\n",
       "hlgt_jart_code+        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl4 = pd.read_excel(ont_DL4_data, sep='\\t',na_values=' ') #Data loaded so that blank excell cells are 'NA'\n",
    "tl4.shape\n",
    "tl4.info()\n",
    "tl4.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: A total of 337 unique HLGT MedDRA terms/codes observed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Summarize Term Level 5 Files (MedDRA SOC files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 354 entries, 0 to 353\n",
      "Data columns (total 2 columns):\n",
      "soc_code     354 non-null int64\n",
      "hlgt_code    354 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 5.6 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "soc_code      27\n",
       "hlgt_code    337\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl5_tl4 = pd.read_excel(ont_DL5_DL4_data, sep='\\t',na_values=' ') #Data loaded so that blank excell cells are 'NA'\n",
    "tl5_tl4.shape\n",
    "tl5_tl4.info()\n",
    "tl5_tl4.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27 entries, 0 to 26\n",
      "Data columns (total 10 columns):\n",
      "soc_code            27 non-null int64\n",
      "soc_name            27 non-null object\n",
      "soc_abbrev          27 non-null object\n",
      "soc_whoart_code+    0 non-null float64\n",
      "soc_harts_code      0 non-null float64\n",
      "soc_costart_sym+    0 non-null float64\n",
      "soc_icd9_code+      0 non-null float64\n",
      "soc_icd9cm_code+    0 non-null float64\n",
      "soc_icd10_code+     0 non-null float64\n",
      "soc_jart_code+      0 non-null float64\n",
      "dtypes: float64(7), int64(1), object(2)\n",
      "memory usage: 2.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "soc_code            27\n",
       "soc_name            27\n",
       "soc_abbrev          27\n",
       "soc_whoart_code+     0\n",
       "soc_harts_code       0\n",
       "soc_costart_sym+     0\n",
       "soc_icd9_code+       0\n",
       "soc_icd9cm_code+     0\n",
       "soc_icd10_code+      0\n",
       "soc_jart_code+       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl5 = pd.read_excel(ont_DL5_data, sep='\\t',na_values=' ') #Data loaded so that blank excell cells are 'NA'\n",
    "tl5.shape\n",
    "tl5.info()\n",
    "tl5.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*APPLICATION NOTE: A total of 27 unique SOC MedDRA terms/codes observed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have confirmed that we can open these MedDRA standard terminology files and have looked over the summaries to make sure that everything looks in order. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
